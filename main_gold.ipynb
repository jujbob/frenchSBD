{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Boundary detection with NER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_newline_to_EOS(file_r, file_w):\n",
    "    with open(file_w, 'w', encoding=\"UTF-8\") as fw:\n",
    "        with open(file_r, 'r', encoding=\"UTF-8\") as fr:\n",
    "            while True:\n",
    "                line = fr.readline()\n",
    "                if not line: break\n",
    "                if line == '\\n':\n",
    "                    fw.write(\"<EOS>\\t<EOS>\\t<EOS>\\t<EOS>\\n\")\n",
    "                else:\n",
    "                    fw.write(line)\n",
    "                    \n",
    "def set_sentence_num(df): \n",
    "\n",
    "    sent_num = 0\n",
    "    df['sent_num'] = sent_num\n",
    "    for idx in range(len(df)):\n",
    "        df['sent_num'][idx] = sent_num\n",
    "        #if df[0][idx]=='.' and df[1][idx]==\"SENT\":\n",
    "        if df[0][idx]=='<EOS>' and df[1][idx]==\"<EOS>\":\n",
    "            sent_num +=1\n",
    "    df.head()\n",
    "    df = df[df[0] != \"<EOS>\"] #get rid of the \"<EOS>\" toekns\n",
    "    print(sent_num)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_file = './data/train.tsv'\n",
    "test_file = './data/test.tsv'\n",
    "train_EOS_file = './data/train_EOS.tsv'\n",
    "test_EOS_file = './data/test_EOS.tsv'\n",
    "#test_file = './data_v1/europarl-sbd-eval.tsv'\n",
    "\n",
    "set_newline_to_EOS(train_file, train_EOS_file)\n",
    "set_newline_to_EOS(test_file, test_EOS_file)\n",
    "train_file = train_EOS_file\n",
    "test_file = test_EOS_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "train_df.head()\n",
    "test_df = pd.read_csv(test_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = set_sentence_num(train_df)\n",
    "test_df = set_sentence_num(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['sent_num'] < 100000]\n",
    "test_df = test_df[test_df['sent_num'] < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_df[train_df['sent_num']==0]\n",
    "token_list =  ' '.join([token for token in sentence[0]])\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSBDataset():\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.idxtob = {'B-SENT': 1}\n",
    "        self.idxtoPOS = {'DET:ART': 0,'NAM': 1,'KON': 2,'PUN': 3,'DET:POS': 4,'NOM': 5,'VER:pres': 6,'PRP': 7,'PRO:PER': 8,'VER:infi': 9,'PRP:det': 10,'VER:simp': 11,'VER:pper': 12,'NUM': 13,'SENT': 14,'ABR': 15,'VER:futu': 16,'PRO:DEM': 17,'ADJ': 18,\n",
    " 'PRO:REL': 19,'PRO:IND': 20,'ADV': 21,'SYM': 22,'PUN:cit': 23,'VER:impf': 24,'VER:subp': 25,'VER:subi': 26,'VER:ppre': 27,'VER:cond': 28,'PRO:POS': 29,'VER:impe': 30}\n",
    "        self.idxtoNER = {'O': 0, 'I-PER': 1, 'I-LOC': 2, 'I-ORG': 3}\n",
    "        \n",
    "        self.padding_idx=0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data['sent_num'].max()\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        sentence = self.data[self.data['sent_num']==item]\n",
    "        token_list =  [token for token in sentence[0]]\n",
    "        sbd_list =  [target for target in sentence[3]]\n",
    "        pos_list =  [target for target in sentence[1]]\n",
    "        ner_list =  [target for target in sentence[2]]\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(token_list),\n",
    "                                            None,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_length,\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length')\n",
    "        \n",
    "        ids = encoded['input_ids']\n",
    "        mask = encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; sbd_ids = [-1]; pos_ids = [self.padding_idx]; ner_ids = [self.padding_idx] # --> CLS token\n",
    "        \n",
    "        for word, sbd, pos, ner in zip(token_list, sbd_list, pos_list, ner_list):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            \n",
    "            sbd_mask = [self.idxtob.get(sbd,0)] + [-1]*(bpe_len-1)\n",
    "            sbd_ids.extend(sbd_mask)\n",
    "            pos_mask = [self.idxtoPOS.get(pos,0)] + [self.padding_idx]*(bpe_len-1)\n",
    "            pos_ids.extend(pos_mask)\n",
    "            ner_mask = [self.idxtoNER.get(ner,0)] + [self.padding_idx]*(bpe_len-1)\n",
    "            ner_ids.extend(ner_mask)\n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0)\n",
    "        bpe_head_mask.extend([0] * (self.max_length - len(bpe_head_mask)))\n",
    "        \n",
    "        sbd_ids.append(-1) # --> END token\n",
    "        sbd_ids.extend([-1] * (self.max_length - len(sbd_ids))) ## --> padding by max_len\n",
    "        pos_ids.append(self.padding_idx) # --> END token\n",
    "        pos_ids.extend([self.padding_idx] * (self.max_length - len(pos_ids))) ## --> padding by max_len\n",
    "        ner_ids.append(self.padding_idx) # --> END token\n",
    "        ner_ids.extend([self.padding_idx] * (self.max_length - len(ner_ids))) ## --> padding by max_len\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "            'sbd_ids': torch.tensor(sbd_ids, dtype=torch.long),\n",
    "            'pos_ids': torch.tensor(pos_ids, dtype=torch.long),\n",
    "            'ner_ids': torch.tensor(ner_ids, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaBaseline(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBaseline, self).__init__()\n",
    "        \n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        o1, o2 = self.bert(ids, mask)\n",
    "        out = self.dropout(o1)\n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "class XLMRobertaBaselineGold(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBaselineGold, self).__init__()\n",
    "        \n",
    "        num_pos = 31\n",
    "        num_ner = 4\n",
    "        pos_dim = 250\n",
    "        ner_dim = 125\n",
    "        \n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.input_dim = 768+pos_dim+ner_dim\n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(self.input_dim, 2)\n",
    "        \n",
    "        self.pos_emb = torch.nn.Embedding(num_pos, pos_dim)\n",
    "        self.ner_emb = torch.nn.Embedding(num_ner, ner_dim)        \n",
    "        \n",
    "    def forward(self, ids, mask, pos_ids, ner_ids):\n",
    "        \n",
    "        o1, o2 = self.bert(ids, mask)\n",
    "        \n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "        ner_emb = self.ner_emb(ner_ids)\n",
    "        concatenated = torch.cat([o1, pos_emb, ner_emb], dim=2)\n",
    "        out = self.dropout(concatenated)        \n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class POS(torch.nn.Module):\n",
    "    def __init__(self, num_pos=31):\n",
    "        super(POS, self).__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, num_pos)\n",
    "        \n",
    "    def forward(self, bert_out):\n",
    "        \n",
    "        o1 = bert_out\n",
    "        out = self.dropout(o1)\n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class NER(torch.nn.Module):\n",
    "    def __init__(self, num_ner=4):\n",
    "        super(NER, self).__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, num_ner)\n",
    "        \n",
    "    def forward(self, bert_out):\n",
    "        \n",
    "        o1 = bert_out\n",
    "        out = self.dropout(o1)\n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class SBD(torch.nn.Module):\n",
    "    def __init__(self, num_sbd=2):\n",
    "        super(SBD, self).__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, num_sbd)\n",
    "        \n",
    "    def forward(self, bert_out):\n",
    "        \n",
    "        o1 = bert_out\n",
    "        out = self.dropout(o1)\n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class XLMRobertaMultiTask(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaMultiTask, self).__init__()\n",
    "        \n",
    "        num_pos = 31\n",
    "        num_ner = 4\n",
    "        num_sbd = 2\n",
    "        pos_dim = 50\n",
    "        ner_dim = 25\n",
    "        \n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        \n",
    "        self.pos = POS(num_pos)\n",
    "        self.ner = NER(num_ner)\n",
    "        self.sbd = SBD(num_sbd)\n",
    "        \n",
    "        self.classfier_pos = torch.nn.Linear(768, 31)\n",
    "        self.classfier_ner = torch.nn.Linear(768, 4)\n",
    "        \n",
    "        self.pos_emb = torch.nn.Embedding(num_pos, pos_dim)\n",
    "        self.ner_emb = torch.nn.Embedding(num_ner, ner_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        o1, o2 = self.bert(ids, mask)\n",
    "        out = o1 #self.dropout(o1)\n",
    "        \n",
    "        #Step1: predict POS tags for the entire toekns\n",
    "        pos_logits = self.pos(out)\n",
    "        ner_logits = self.ner(out)\n",
    "        sbd_logits = self.sbd(out)\n",
    "        \n",
    "        #pos_idx = torch.argmax(pos_logits)\n",
    "        #ner_idx = torch.argmax(ner_logits)\n",
    "        \n",
    "        #pos_emb = self.pos_emb(pos_idx)\n",
    "        #ner_emb = self.ner_emb(ner_idx)\n",
    "        \n",
    "        \n",
    "        return sbd_logits, pos_logits, ner_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 510\n",
    "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "train_dataset = FSBDataset(train_df, tokenizer, MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, num_workers=4, batch_size=120)\n",
    "test_dataset = FSBDataset(test_df, tokenizer, MAX_LEN)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, num_workers=4, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = XLMRobertaBaseline()\n",
    "model = XLMRobertaBaselineGold()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(params=model.parameters(), lr=0.000005)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "\n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    #print(\"incidence_nopad\", incidence_nopad)\n",
    "    \n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad == 1)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        p = count_correct_p/count_active_tokens_p\n",
    "    except ZeroDivisionError:\n",
    "        p = 0\n",
    "    \n",
    "\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag==1)\n",
    "    np_total_pred_r = np_total_pred[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag[ids_sb_pred_r]\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        r = count_correct_r/count_active_tokens_r\n",
    "    except ZeroDivisionError:\n",
    "        r = 0\n",
    "    \n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        f1 = 2*(p*r) / (p+r)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    \n",
    "\n",
    "    print(\"F1:\", f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logists = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['pos_ids'].cuda(), batch['ner_ids'].cuda())\n",
    "\n",
    "        b,s,l = logists.size()\n",
    "        loss = loss_fn(logists.view(b*s,l), batch['sbd_ids'].cuda().view(b*s))\n",
    "        total_loss.append(loss.item())\n",
    "        total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "        total_targ.extend(batch['sbd_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "    count_active_tokens = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    f1_score(total_pred, total_targ)\n",
    "\n",
    "\n",
    "    \n",
    "def dev_loop_fn(dev_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.eval()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    total_middle_pred = []\n",
    "    total_middle_targ = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            logists = model(batch['ids'].cuda(), batch['mask'].cuda(), batch['pos_ids'].cuda(), batch['ner_ids'].cuda())\n",
    "\n",
    "            b,s,l = logists.size()\n",
    "            loss = loss_fn(logists.view(b*s,l), batch['sbd_ids'].cuda().view(b*s))\n",
    "            total_loss.append(loss.item())\n",
    "            total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "            total_targ.extend(batch['sbd_ids'].cuda().view(b*s).cpu().tolist())\n",
    "            \n",
    "\n",
    "            logists2 = logists[:,2:,]\n",
    "            b,s,l = logists2.size()\n",
    "            #print(b,s,l)\n",
    "            total_middle_pred.extend(torch.argmax(logists2.contiguous().view(b*s,l), 1).cpu().tolist())\n",
    "            total_middle_targ.extend(batch['sbd_ids'][:,2:].cuda().contiguous().view(b*s).cpu().tolist())\n",
    "\n",
    "    print(\"the number of total pred and targ\", len(total_pred), len(total_targ))\n",
    "    f1_score(total_pred, total_targ)\n",
    "    f1_score(total_middle_pred, total_middle_targ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(200):\n",
    "    train_loop_fn(train_loader, model, optimizer)\n",
    "    dev_loop_fn(test_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loop_fn(test_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4 on Python 3.6 (CUDA 10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
