{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Boundary detection with NER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/test.tsv'\n",
    "test_file = './data/test.tsv'\n",
    "#test_file = './data_v1/europarl-sbd-eval.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>B-SENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cérémonie</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aura</td>\n",
       "      <td>VER:futu</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lieu</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>le</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  2       3\n",
       "0         La   DET:ART  O  B-SENT\n",
       "1  cérémonie       NOM  O       O\n",
       "2       aura  VER:futu  O       O\n",
       "3       lieu       NOM  O       O\n",
       "4         le   DET:ART  O       O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "train_df.head()\n",
    "#train_df = pd.read_excel(train_file, delimiter='\\t', encoding='UTF-8', error_bad_lines=False, header=None)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>B-SENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cérémonie</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aura</td>\n",
       "      <td>VER:futu</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lieu</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>le</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  2       3\n",
       "0         La   DET:ART  O  B-SENT\n",
       "1  cérémonie       NOM  O       O\n",
       "2       aura  VER:futu  O       O\n",
       "3       lieu       NOM  O       O\n",
       "4         le   DET:ART  O       O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sentence_num(df): \n",
    "\n",
    "    sent_num = 0\n",
    "    df['sent_num'] = sent_num\n",
    "    for idx in range(len(df)):\n",
    "        df['sent_num'][idx] = sent_num\n",
    "        if df[0][idx]=='.' and df[1][idx]==\"SENT\":\n",
    "            sent_num +=1\n",
    "    df.head()\n",
    "    print(sent_num)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "train_df = set_sentence_num(train_df)\n",
    "test_df = set_sentence_num(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['sent_num'] < 100]\n",
    "test_df = test_df[test_df['sent_num'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La famille remercie toutes les personnes qui s' associeront à sa peine .\n"
     ]
    }
   ],
   "source": [
    "sentence = train_df[train_df['sent_num']==2]\n",
    "token_list =  ' '.join([token for token in sentence[0]])\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSBDataset():\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.idxtob = {'B-SENT': 1}\n",
    "        self.idxtoPOS = {'DET:ART': 0,'NAM': 1,'KON': 2,'PUN': 3,'DET:POS': 4,'NOM': 5,'VER:pres': 6,'PRP': 7,'PRO:PER': 8,'VER:infi': 9,'PRP:det': 10,'VER:simp': 11,'VER:pper': 12,'NUM': 13,'SENT': 14,'ABR': 15,'VER:futu': 16,'PRO:DEM': 17,'ADJ': 18,\n",
    " 'PRO:REL': 19,'PRO:IND': 20,'ADV': 21,'SYM': 22,'PUN:cit': 23,'VER:impf': 24,'VER:subp': 25,'VER:subi': 26,'VER:ppre': 27,'VER:cond': 28,'PRO:POS': 29,'VER:impe': 30}\n",
    "        self.idxtoNER = {'I-LOC': 0, 'I-PER': 1, 'O': 2, 'I-ORG': 3}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data['sent_num'].max()\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        sentence = train_df[train_df['sent_num']==item]\n",
    "        token_list =  [token for token in sentence[0]]\n",
    "        target_list =  [target for target in sentence[3]]\n",
    "        target_ids_list =  [1 if token==\"B-SENT\" else 0 for token in sentence[3]]\n",
    "        pos_ids = [self.idxtoPOS.get(pos) for pos in sentence[1]]\n",
    "        ner_ids = [self.idxtoNER.get(ner) for ner in sentence[2]]\n",
    "        \n",
    "\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(token_list),\n",
    "                                            None,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_length,\n",
    "                                            truncation=True,\n",
    "                                            pad_to_max_length=True)\n",
    "        \n",
    "        ids = encoded['input_ids']\n",
    "        mask = encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; upos_ids = [-1] # --> CLS token\n",
    "        \n",
    "        for word, target in zip(token_list, target_list):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            upos_mask = [self.idxtob.get(target,0)] + [-1]*(bpe_len-1)\n",
    "            upos_ids.extend(upos_mask)\n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0); upos_ids.append(-1) # --> END token\n",
    "        bpe_head_mask.extend([0] * (self.max_length - len(bpe_head_mask))); upos_ids.extend([-1] * (self.max_length - len(upos_ids))) ## --> padding by max_len\n",
    "\n",
    "        \n",
    "        print(target_ids_list)\n",
    "        print(pos_ids)\n",
    "        print(ner_ids)\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            #'target': torch.tensor(target_list, dtype=torch.long),\n",
    "            'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "            'target_ids': torch.tensor(upos_ids, dtype=torch.long),\n",
    "            'pos_ids': torch.tensor(pos_ids, dtype=torch.long)\n",
    "            #'ner_ids': torch.tensor(ner_ids, dtype=torch.long)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_char, dim_char, hidden_size=200, layer_size=1, drop_out=0.3, mlp_size=100, num_out=100):\n",
    "        super(CharModel, self).__init__()\n",
    "\n",
    "        self.dim_char = dim_char \n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size = layer_size\n",
    "        self.dropout = 0.3\n",
    "        self.directions = 2\n",
    "        self.mlp_use = True\n",
    "        self.char_out = num_out if self.mlp_use else (self.hidden_size * self.directions * 2)\n",
    "\n",
    "        self.char_emb = nn.Embedding(num_char, dim_char, padding_idx=0)\n",
    "        self.char_LSTM = nn.LSTM(dim_char, self.hidden_size, self.layer_size, dropout=self.dropout, bidirectional=True)\n",
    "        self.char_mlp = MLP(self.hidden_size * self.directions * 2, self.char_out, 0.3)\n",
    "        self.drop_out = nn.Dropout(p=self.dropout)\n",
    " #       self.att_score = nn.Softmax(dim=1)\n",
    " #       self.char_linear = nn.Linear(self.mlp_size * self.hidden_size * self.directions, num_out, bias=True)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, char_seq, char_len):\n",
    "        \n",
    "        pad_seqs = char_seq\n",
    "        char_vecs = self.char_emb(pad_seqs)\n",
    "        char_len_list = np.array([len(sentence) for sentence in char_vecs])   #  np.array(char_len)\n",
    "        \n",
    "#        self.char_LSTM.flatten_parameters() # To apply multi-gpu excution, https://discuss.pytorch.org/t/rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memory/6011/14\n",
    "#        total_length = input_emb.size(1) # To apply multi-gpu excution, https://pytorch.org/docs/stable/notes/faq.html\n",
    "\n",
    "        h0 = self.init_hidden(char_vecs.size(0)) # initial state of LSTM\n",
    "        global_indices = np.argsort(-np.array(char_len_list)).astype(np.int64) #sorting based on seq_len\n",
    "        char_len_list_ordered = char_len_list[global_indices] #Sorting \"char_len_list\" based on the indices\n",
    "        char_vecs_ordered = torch.stack([char_vecs[order] for order in global_indices]) #Stacking batches as an embedding based on dec order                \n",
    "        char_sentence_packed = nn.utils.rnn.pack_padded_sequence(char_vecs_ordered, char_len_list_ordered, batch_first=True)\n",
    "        char_lstm_batch_out = self.char_LSTM(char_sentence_packed, h0)[0] # (Batch * seq_len x  2*LSTM_hidden)\n",
    "        char_lstm_batch_out = nn.utils.rnn.pad_packed_sequence(char_lstm_batch_out, batch_first=True)[0] # (Batch x seq_len x 2*LSTM_hidden)\n",
    "        char_lstm_batch_out = char_lstm_batch_out.index_select(dim=0, index=_model_var(self, torch.from_numpy(np.argsort(global_indices).astype(np.int64)))) #make it back unorder\n",
    "\n",
    "        batch_size = char_vecs.size(0)\n",
    "        word_len = [len(sentence) for sentence in char_len]\n",
    "        word_max = max(word_len) \n",
    "        char_max = max(char_len_list)\n",
    "\n",
    "        char_feature = torch.zeros(batch_size, word_max, self.hidden_size * self.directions *2 ).to(self.device)\n",
    "        mask_s = torch.zeros(batch_size, char_max, dtype=torch.uint8).to(self.device)\n",
    "        mask_e = torch.zeros(batch_size, char_max, dtype=torch.uint8).to(self.device)\n",
    "        for batch_idx in range(batch_size):\n",
    "            ch_start_idx, ch_end_idx = 0,0\n",
    "            for token_idx, length in enumerate(char_len[batch_idx]):\n",
    "                token_lenth = length\n",
    "                ch_end_idx = ch_start_idx + token_lenth \n",
    "                mask_s[batch_idx, ch_start_idx] = 1 #char_global_lstm_batch_out[0, 0:6, :] ==> *root*\n",
    "                mask_e[batch_idx, ch_end_idx] = 1 #char_global_lstm_batch_out[0, 0:6, :] ==> *root*\n",
    "                ch_start_idx = ch_end_idx + 1 # +1 == white space KKL\n",
    "\n",
    "        char_s_masked = char_lstm_batch_out[mask_s,:]\n",
    "        char_e_masked = char_lstm_batch_out[mask_e,:]\n",
    "        char_masked = torch.cat((char_s_masked, char_e_masked),-1)\n",
    "        \n",
    "        w_idx = 0\n",
    "        for batch, seq in zip(range(batch_size), word_len):\n",
    "            char_feature[batch,:seq,:] = char_masked[w_idx:w_idx+seq,:]\n",
    "            w_idx += seq\n",
    "            \n",
    "        char_feature = self.char_mlp(char_feature) if self.mlp_use else char_feature\n",
    "\n",
    "        return char_feature\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''Create initial hidden state of zeros: 2-tuple of num_layers x batch size x hidden dim'''\n",
    "        num_layers = self.layer_size * self.directions\n",
    "        init = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(self.device)\n",
    "        init = torch.nn.init.xavier_normal_(init)\n",
    "        h0 = (init, init.clone())\n",
    "\n",
    "\n",
    "\n",
    "class XLMRobertaBaseline(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBaseline, self).__init__()\n",
    "        \n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, 2)\n",
    "        \n",
    "        num_pos = 31\n",
    "        num_ner = 4\n",
    "        pos_dim = 50\n",
    "        ner_dim = 25\n",
    "        \n",
    "        self.classfier_pos = torch.nn.Linear(768, 31)\n",
    "        self.classfier_ner = torch.nn.Linear(768, 4)\n",
    "        \n",
    "        self.pos_emb = torch.nn.Embedding(num_pos, pos_dim)\n",
    "        self.ner_emb = torch.nn.Embedding(num_ner, ner_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        o1, o2 = self.bert(ids, mask)\n",
    "        out = self.dropout(o1)\n",
    "        \n",
    "        #Step1: predict POS tags for the entire toekns\n",
    "        pos_logits = self.classfier_pos(out)\n",
    "        ner_logits = self.classfier_ner(out)\n",
    "        \n",
    "        pos_idx = torch.argmax(pos_logits)\n",
    "        ner_idx = torch.argmax(ner_logits)\n",
    "        \n",
    "        pos_emb = self.pos_emb(pos_idx)\n",
    "        ner_emb = self.ner_emb(ner_idx)\n",
    "        \n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 640\n",
    "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "train_dataset = FSBDataset(train_df, tokenizer, MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, num_workers=1, batch_size=2)\n",
    "test_dataset = FSBDataset(test_df, tokenizer, MAX_LEN)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, num_workers=1, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLMRobertaBaseline()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(params=model.parameters(), lr=0.000005)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "    \n",
    "    #print(\"total_pred\", total_pred, len(total_pred))\n",
    "    #print(\"total_targ\", total_targ, len(total_targ))\n",
    "    \n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    #print(\"incidence_nopad\", incidence_nopad)\n",
    "    \n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad == 1)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    '''\n",
    "    np_total_pred_incid = np_total_pred[incidence_p]\n",
    "    print(\"np_total_pred_incid\", np_total_pred_incid)\n",
    "    ids_sb_pred_p = np.where(np_total_pred_incid==1)\n",
    "    np_total_pred_p = np_total_pred_incid[ids_sb_pred_p]\n",
    "    np_total_tag_p = np_total_tag[ids_sb_pred_p]\n",
    "    \n",
    "    print(\"ids_sb_pred_p\", ids_sb_pred_p)\n",
    "    print(\"np_total_pred_p\", np_total_pred_p)\n",
    "    print(\"np_total_tag_p\", np_total_tag_p)\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_p)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_p==np_total_tag_p) == True)\n",
    "    '''\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    p = count_correct_p/count_active_tokens_p\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag==1)\n",
    "    np_total_pred_r = np_total_pred[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag[ids_sb_pred_r]\n",
    "    \n",
    "    #print(\"ids_sb_pred_r\", ids_sb_pred_r)\n",
    "    #print(\"np_total_pred_r\", np_total_pred_r)\n",
    "    #print(\"np_total_tag_r\", np_total_tag_r)\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    r = count_correct_r/count_active_tokens_r\n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    #F1\n",
    "    f1 = 2*(p*r) / (p+r)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    #count_active_tokens_recall = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #print(\"count_active_tokens_recall\", count_active_tokens_recall)\n",
    "    #count_active_tokens_precision = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    \n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"count_correct\",count_correct)\n",
    "    #print(\"ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch['ids'], len(batch['ids']), batch['ids'].size() )\n",
    "        #print(batch['mask'], len(batch['mask']))\n",
    "        #print(batch['bpe_head_mask'], len(batch['bpe_head_mask']))\n",
    "        #print(batch['upos_ids'], len(batch['upos_ids']))\n",
    "\n",
    "        logists = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "        #print(logists, logists.size())\n",
    "        #print(batch['upos_ids'], batch['upos_ids'].size())\n",
    "        #print(logists.view(45,9), logists.view(45,9).size())\n",
    "        #print(batch['upos_ids'].view(45), batch['upos_ids'].view(45).size())\n",
    "        b,s,l = logists.size()\n",
    "        loss = loss_fn(logists.view(b*s,l), batch['target_ids'].cuda().view(b*s))\n",
    "        total_loss.append(loss.item())\n",
    "        total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "        total_targ.extend(batch['target_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        #print(\"batch\",batch)\n",
    "        #break\n",
    "    #print(total_pred, len(total_pred))\n",
    "    #print(total_targ, len(total_targ))\n",
    "    count_active_tokens = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"TRAINING ACCURACY:\", count_correct/count_active_tokens)\n",
    "    f1_score(total_pred, total_targ)\n",
    "    #f1_score(total_pred[2:], total_targ[2:])\n",
    "    #print(count_active_tokens)\n",
    "    #print(count_correct)\n",
    "\n",
    "    \n",
    "def dev_loop_fn(dev_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.eval()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    total_middle_pred = []\n",
    "    total_middle_targ = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            logists = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "            b,s,l = logists.size()\n",
    "            #print(b,s,l)\n",
    "            loss = loss_fn(logists.view(b*s,l), batch['target_ids'].cuda().view(b*s))\n",
    "            total_loss.append(loss.item())\n",
    "            total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "            total_targ.extend(batch['target_ids'].cuda().view(b*s).cpu().tolist())\n",
    "            \n",
    "\n",
    "            logists2 = logists[:,2:,]\n",
    "            b,s,l = logists2.size()\n",
    "            #print(b,s,l)\n",
    "            total_middle_pred.extend(torch.argmax(logists2.contiguous().view(b*s,l), 1).cpu().tolist())\n",
    "            total_middle_targ.extend(batch['target_ids'][:,2:].cuda().contiguous().view(b*s).cpu().tolist())\n",
    "\n",
    "    f1_score(total_pred, total_targ)\n",
    "    f1_score(total_middle_pred, total_middle_targ)\n",
    "\n",
    "    \n",
    "    \n",
    "    #count_active_tokens = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"TESTING ACC:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 5, 6, 12, 7, 4, 5, 12, 14]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 5, 7, 5, 16, 12, 0, 5, 13, 5, 13, 3, 7, 5, 3, 7, 0, 5, 7, 1, 14]\n",
      "[2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[4, 5, 16, 5, 5, 13, 5, 3, 7, 5, 3, 7, 0, 5, 7, 5, 3, 12, 7, 0, 5, 10, 5, 14]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [9] at entry 0 and [21] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-32141ec03c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdev_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fca50a58ba4a>\u001b[0m in \u001b[0;36mtrain_loop_fn\u001b[0;34m(train_loader, model, optimizer, DEVICE, scheduler)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(batch['ids'], len(batch['ids']), batch['ids'].size() )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/work/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [9] at entry 0 and [21] at entry 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[5, 1, 6, 7, 0, 5, 18, 7, 0, 5, 7, 1, 14]\n",
      "[2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[5, 1, 6, 10, 5, 1, 3, 13, 3, 5, 10, 1, 7, 1, 3, 21, 10, 5, 3, 14]\n",
      "[2, 1, 2, 2, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 5, 6, 7, 5, 20, 0, 5, 19, 16, 5, 7, 4, 5, 14]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(100):\n",
    "    train_loop_fn(train_loader, model, optimizer)\n",
    "    break\n",
    "    dev_loop_fn(test_loader, model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for batch in train_loader:\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(batch)\n",
    "    logits = model(batch['ids'], batch['mask'])\n",
    "    print(logits)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_df.groupby('sent_num').max()\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train_df[train_df['sent_num'] == 12025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voca(self, file):\n",
    "    \n",
    "    with open(file, encoding='UTF-8') as f:\n",
    "        f.readline()\n",
    "    \n",
    "    return voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, encoding='UTF-8') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist = set(list(test_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "charsCount = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = list(train_df[2].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dic = {pos:idx for idx, pos in enumerate(pos_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_dist:\n",
    "    charsCount.update(list(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charsCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4 on Python 3.6 (CUDA 10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
