{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Boundary detection with NER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train.tsv'\n",
    "test_file = './data/test.tsv'\n",
    "#test_file = './data_v1/europarl-sbd-eval.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>B-SENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sables-d'Olonne</td>\n",
       "      <td>NAM</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chaume</td>\n",
       "      <td>NAM</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philippe</td>\n",
       "      <td>NAM</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-SENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0        1      2       3\n",
       "0              Les  DET:ART  I-LOC  B-SENT\n",
       "1  Sables-d'Olonne      NAM  I-LOC       O\n",
       "2               La  DET:ART  I-LOC       O\n",
       "3           Chaume      NAM  I-LOC       O\n",
       "4         Philippe      NAM  I-PER  B-SENT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "train_df.head()\n",
    "#train_df = pd.read_excel(train_file, delimiter='\\t', encoding='UTF-8', error_bad_lines=False, header=None)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>B-SENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cérémonie</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aura</td>\n",
       "      <td>VER:futu</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lieu</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>le</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  2       3\n",
       "0         La   DET:ART  O  B-SENT\n",
       "1  cérémonie       NOM  O       O\n",
       "2       aura  VER:futu  O       O\n",
       "3       lieu       NOM  O       O\n",
       "4         le   DET:ART  O       O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file, delimiter='\\t', engine='python', encoding='UTF-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sentence_num(df): \n",
    "\n",
    "    sent_num = 0\n",
    "    df['sent_num'] = sent_num\n",
    "    for idx in range(len(df)):\n",
    "        df['sent_num'][idx] = sent_num\n",
    "        if df[0][idx]=='.' and df[1][idx]==\"SENT\":\n",
    "            sent_num +=1\n",
    "    df.head()\n",
    "    print(sent_num)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27984\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "train_df = set_sentence_num(train_df)\n",
    "test_df = set_sentence_num(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['sent_num'] < 10000]\n",
    "test_df = test_df[test_df['sent_num'] < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cérémonie d' au-revoir aura lieu samedi 29 novembre 2014 , à 15h30 , au crématorium d' Olonne-sur-Mer .\n"
     ]
    }
   ],
   "source": [
    "sentence = train_df[train_df['sent_num']==2]\n",
    "token_list =  ' '.join([token for token in sentence[0]])\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSBDataset():\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.idxtob = {'B-SENT': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data['sent_num'].max()\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        sentence = train_df[train_df['sent_num']==item]\n",
    "        token_list =  [token for token in sentence[0]]\n",
    "        target_list =  [target for target in sentence[3]]\n",
    "        target_ids_list =  [1 if token==\"B-SENT\" else 0 for token in sentence[3]]\n",
    "        \n",
    "        encoded = self.tokenizer.encode_plus(' '.join(token_list),\n",
    "                                            None,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_length,\n",
    "                                            truncation=True,\n",
    "                                            pad_to_max_length=True)\n",
    "        \n",
    "        ids = encoded['input_ids']\n",
    "        mask = encoded['attention_mask']\n",
    "        \n",
    "        bpe_head_mask = [0]; upos_ids = [-1] # --> CLS token\n",
    "        \n",
    "        for word, target in zip(token_list, target_list):\n",
    "            bpe_len = len(self.tokenizer.tokenize(word))\n",
    "            head_mask = [1] + [0]*(bpe_len-1)\n",
    "            bpe_head_mask.extend(head_mask)\n",
    "            upos_mask = [self.idxtob.get(target,0)] + [-1]*(bpe_len-1)\n",
    "            upos_ids.extend(upos_mask)\n",
    "            #print(\"head_mask\", head_mask)\n",
    "        \n",
    "        bpe_head_mask.append(0); upos_ids.append(-1) # --> END token\n",
    "        bpe_head_mask.extend([0] * (self.max_length - len(bpe_head_mask))); upos_ids.extend([-1] * (self.max_length - len(upos_ids))) ## --> padding by max_len\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            #'target': torch.tensor(target_list, dtype=torch.long),\n",
    "            'bpe_head_mask': torch.tensor(bpe_head_mask, dtype=torch.long),\n",
    "            'target_ids': torch.tensor(upos_ids, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaBaseline(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBaseline, self).__init__()\n",
    "        \n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.dropout = torch.nn.Dropout(0.33)\n",
    "        self.classfier = torch.nn.Linear(768, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        o1, o2 = self.bert(ids, mask)\n",
    "        out = self.dropout(o1)\n",
    "        logits = self.classfier(out)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 640\n",
    "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "train_dataset = FSBDataset(train_df, tokenizer, MAX_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, num_workers=4, batch_size=10)\n",
    "test_dataset = FSBDataset(test_df, tokenizer, MAX_LEN)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, num_workers=4, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLMRobertaBaseline()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(params=model.parameters(), lr=0.000005)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(total_pred, total_targ):\n",
    "    \n",
    "    p = 0 # (retrived SB and real SB) / retrived SB  # The percentage of (the number of correct predictions) / (the number of predction that system predicts as B-SENT)\n",
    "    r = 0\n",
    "    f1= 0\n",
    "    \n",
    "    #print(\"total_pred\", total_pred, len(total_pred))\n",
    "    #print(\"total_targ\", total_targ, len(total_targ))\n",
    "    \n",
    "    np_total_pred = np.array(total_pred)\n",
    "    np_total_tag = np.array(total_targ)\n",
    "    \n",
    "    #precision\n",
    "    incidence_nopad = np.where(np_total_tag != -1) ## eliminate paddings\n",
    "    #print(\"incidence_nopad\", incidence_nopad)\n",
    "    \n",
    "    np_total_pred_nopad = np_total_pred[incidence_nopad]\n",
    "    np_total_tag_nopad = np_total_tag[incidence_nopad]\n",
    "    \n",
    "    incidence_nopad_sb = np.where(np_total_pred_nopad == 1)\n",
    "    np_total_pred_nopad_sb = np_total_pred_nopad[incidence_nopad_sb]\n",
    "    np_total_tag_nopad_sb = np_total_tag_nopad[incidence_nopad_sb]\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_nopad_sb)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_nopad_sb==np_total_tag_nopad_sb) == True)\n",
    "    \n",
    "    '''\n",
    "    np_total_pred_incid = np_total_pred[incidence_p]\n",
    "    print(\"np_total_pred_incid\", np_total_pred_incid)\n",
    "    ids_sb_pred_p = np.where(np_total_pred_incid==1)\n",
    "    np_total_pred_p = np_total_pred_incid[ids_sb_pred_p]\n",
    "    np_total_tag_p = np_total_tag[ids_sb_pred_p]\n",
    "    \n",
    "    print(\"ids_sb_pred_p\", ids_sb_pred_p)\n",
    "    print(\"np_total_pred_p\", np_total_pred_p)\n",
    "    print(\"np_total_tag_p\", np_total_tag_p)\n",
    "    \n",
    "    count_active_tokens_p = len(np_total_pred_p)\n",
    "    count_correct_p = np.count_nonzero((np_total_pred_p==np_total_tag_p) == True)\n",
    "    '''\n",
    "    \n",
    "    print(\"count_correct_p\", count_correct_p)\n",
    "    print(\"count_active_tokens_p\", count_active_tokens_p)\n",
    "    \n",
    "    p = count_correct_p/count_active_tokens_p\n",
    "    print(\"precision:\", p)\n",
    "\n",
    "    \n",
    "    #recall\n",
    "    ids_sb_pred_r = np.where(np_total_tag==1)\n",
    "    np_total_pred_r = np_total_pred[ids_sb_pred_r]\n",
    "    np_total_tag_r = np_total_tag[ids_sb_pred_r]\n",
    "    \n",
    "    #print(\"ids_sb_pred_r\", ids_sb_pred_r)\n",
    "    #print(\"np_total_pred_r\", np_total_pred_r)\n",
    "    #print(\"np_total_tag_r\", np_total_tag_r)\n",
    "    \n",
    "    count_active_tokens_r = len(np_total_pred_r)\n",
    "    count_correct_r = np.count_nonzero((np_total_pred_r==np_total_tag_r) == True)\n",
    "    \n",
    "    print(\"count_active_tokens_r\", count_active_tokens_r)\n",
    "    print(\"count_correct_r\", count_correct_r)\n",
    "    \n",
    "    r = count_correct_r/count_active_tokens_r\n",
    "    print(\"recall:\", r)\n",
    "    \n",
    "    \n",
    "    #F1\n",
    "    f1 = 2*(p*r) / (p+r)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    #count_active_tokens_recall = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #print(\"count_active_tokens_recall\", count_active_tokens_recall)\n",
    "    #count_active_tokens_precision = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    \n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"count_correct\",count_correct)\n",
    "    #print(\"ACCURACY:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch['ids'], len(batch['ids']), batch['ids'].size() )\n",
    "        #print(batch['mask'], len(batch['mask']))\n",
    "        #print(batch['bpe_head_mask'], len(batch['bpe_head_mask']))\n",
    "        #print(batch['upos_ids'], len(batch['upos_ids']))\n",
    "\n",
    "        logists = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "        #print(logists, logists.size())\n",
    "        #print(batch['upos_ids'], batch['upos_ids'].size())\n",
    "        #print(logists.view(45,9), logists.view(45,9).size())\n",
    "        #print(batch['upos_ids'].view(45), batch['upos_ids'].view(45).size())\n",
    "        b,s,l = logists.size()\n",
    "        loss = loss_fn(logists.view(b*s,l), batch['target_ids'].cuda().view(b*s))\n",
    "        total_loss.append(loss.item())\n",
    "        total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "        total_targ.extend(batch['target_ids'].cuda().view(b*s).cpu().tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        #print(\"batch\",batch)\n",
    "        #break\n",
    "    #print(total_pred, len(total_pred))\n",
    "    #print(total_targ, len(total_targ))\n",
    "    count_active_tokens = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"TRAINING ACCURACY:\", count_correct/count_active_tokens)\n",
    "    f1_score(total_pred, total_targ)\n",
    "    #f1_score(total_pred[2:], total_targ[2:])\n",
    "    #print(count_active_tokens)\n",
    "    #print(count_correct)\n",
    "\n",
    "    \n",
    "def dev_loop_fn(dev_loader, model, optimizer, DEVICE=None, scheduler=None):\n",
    "    model.eval()\n",
    "    \n",
    "    total_pred = []\n",
    "    total_targ = []\n",
    "    total_loss = []\n",
    "    total_middle_pred = []\n",
    "    total_middle_targ = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(dev_loader), total=len(dev_loader)):\n",
    "\n",
    "            logists = model(batch['ids'].cuda(), batch['mask'].cuda())\n",
    "            b,s,l = logists.size()\n",
    "            #print(b,s,l)\n",
    "            loss = loss_fn(logists.view(b*s,l), batch['target_ids'].cuda().view(b*s))\n",
    "            total_loss.append(loss.item())\n",
    "            total_pred.extend(torch.argmax(logists.view(b*s,l), 1).cpu().tolist())\n",
    "            total_targ.extend(batch['target_ids'].cuda().view(b*s).cpu().tolist())\n",
    "            \n",
    "\n",
    "            logists2 = logists[:,2:,]\n",
    "            b,s,l = logists2.size()\n",
    "            #print(b,s,l)\n",
    "            total_middle_pred.extend(torch.argmax(logists2.contiguous().view(b*s,l), 1).cpu().tolist())\n",
    "            total_middle_targ.extend(batch['target_ids'][:,2:].cuda().contiguous().view(b*s).cpu().tolist())\n",
    "\n",
    "    f1_score(total_pred, total_targ)\n",
    "    f1_score(total_middle_pred, total_middle_targ)\n",
    "\n",
    "    \n",
    "    \n",
    "    #count_active_tokens = np.count_nonzero(np.array(total_targ) > -1)\n",
    "    #count_correct = np.count_nonzero((np.array(total_pred)==np.array(total_targ)) == True)\n",
    "    #print(\"TESTING ACC:\", count_correct/count_active_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(100):\n",
    "    train_loop_fn(train_loader, model, optimizer)\n",
    "    dev_loop_fn(test_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for batch in train_loader:\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(batch)\n",
    "    logits = model(batch['ids'], batch['mask'])\n",
    "    print(logits)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_df.groupby('sent_num').max()\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train_df[train_df['sent_num'] == 12025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4 on Python 3.6 (CUDA 10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
